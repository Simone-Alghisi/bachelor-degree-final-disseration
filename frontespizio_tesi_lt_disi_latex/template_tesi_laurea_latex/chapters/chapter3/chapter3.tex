\graphicspath{{chapters/chapter3/img/}}

\chapter{Methods}
\label{cha:methods}

This chapter aims to describe all the different methodologies used to analyze the data collected during the previous phase and explained in \Cref{cha:data}. 

The scope of the project is to understand and measure the emotions of the users through sentiment analysis. In particular, we would like to identify, given a certain set of tweets scattered across our considered period of time, when the users conveyed more feelings and which was the emotion expressed the most (e.g. from the Italian tweets it is possible to notice a peak of anger on 21st of February 2020).

Given the heterogeneity of the tweets, we would also like to conduct a more in-dept analysis, by considering the users, over the whole time frame, based on their gender and also on their location.

\section{Lexicons}
\label{lexicons}

Until now we have discussed that we would like to identify and quantify the emotions expressed in the tweets, but we still lack a way to achieve this result. During the project I have used \textit{Lexicons} for this particular task.

The idea behind lexicons is quite simple: we take a particular word in our dictionary, and we assign zero or more emotions or sentiments to it.
\begin{figure}[H]
	\centering
	\begin{tikzpicture}
		\filldraw (-2,3) node[RectObject, inner xsep=1cm, dashed](pandemic){pandemic};
		\draw (-2,5) node[RectObject, inner xsep=1cm, draw=gray](hospital){hospital};
		\filldraw (8,1) node[RectObject, inner xsep=1cm, line width=0.3mm](negative){negative};
	\filldraw (8,3) node[RectObject, inner xsep=1cm, draw=purple, line width=0.3mm](fear){fear};
		\filldraw (8,5) node[RectObject, inner xsep=1cm, draw=blue, line width=0.3mm](sadness){sadness};
		\filldraw (8,7) node[RectObject, inner xsep=1cm, draw=green, line width=0.3mm](trust){trust};
		\draw[arrow, dashed] (pandemic.east)--(negative.west);
		\draw[arrow, dashed] (pandemic.east)--(fear.west);
		\draw[arrow, dashed] (pandemic.east)--(sadness.west);
		\draw[arrow, draw=gray] (hospital.east)--(trust.west);
		\draw[arrow, draw=gray] (hospital.east)--(fear.west);
		\draw[arrow, draw=gray] (hospital.east)--(sadness.west);
	\end{tikzpicture}
	\caption{Word-emotions/sentiments association}
	\label{fig:word-association}
\end{figure}

However, if we look at \Cref{fig:word-association}, we can notice that, while the word pandemic is associated with only negative emotions/sentiments, the word hospital is associated with fear and sadness but, at the same time, with trust. While this is technically correct, because our lexicon does not know in which context the word is used (i.e. it must consider all the possible meanings), it also introduces a bias.

To simplify even more, even a negation can totally change the meaning of a particular quote:

\begin{center}
	I am fine / I am \underline{not} fine
\end{center}

For this reason, results obtained should be checked and contextualized to get a clear understanding of the situation. 

\paragraph{EmoLex}

For the research, I have used the NRC Word-Emotion Association Lexicon (aka EmoLex) for emotion detection~\cite{ncrwebsite}. EmoLex is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive).

The peculiarity of this lexicon is the fact that, even if it has been designed for the analysis of English words, it has been translated in over one hundred languages using Google Translates. Given that, the number of different languages identified in the dataset is more than 60, this particular dictionary could perfectly fit our problem.

\section{Data organization}
\label{sub:data-org}

Before performing emotion detection on our tweets, we decided to define a new data organization strategy. In particular,

\begin{itemize}
	\item at the top layer, we sorted the tweets into LANGUAGE folders using the \texttt{lang} field of the json object
	\item then, into YEAR-MONTH folders
	\item finally, we grouped the tweets into files with a prefix “coronavirus-tweet-id-” followed by YEAR-MONTH-DAY
\end{itemize}

The idea behind this partition, aside from considering tweets of the same language, was to get rid of the per hour aggregation: for the purpose of the project, this kind of granularity is simply too much. Given that, we have preferred to aggregate in a single file the tweets posted on the same day.

\Cref{fig:tweet-org-1} shows a visual representation of the new data organization: now it is possible to access directly to tweets of the same language posted on a particular date.

\begin{figure}[H]
	\dirtree{%
		.1 / .
		.2 en.
		.3 2020-01.
		.4 coronavirus-tweet-2020-01-21.json.gz .
		.4 coronavirus-tweet-2020-01-22.json.gz .
		.4 \ldots .
		.4 coronavirus-tweet-2020-01-30.json.gz .
		.4 coronavirus-tweet-2020-01-31.json.gz .
		.3 \ldots .
		.3 2021-03 .
		.4 coronavirus-tweet-2021-03-01.json.gz .
		.4 \ldots .
		.4 coronavirus-tweet-2021-03-31.json.gz .
		.2 it .
		.2 \ldots .
	}
	\caption{First tweets organization}
	\label{fig:tweet-org-1}
\end{figure}

However, this first data organization strategy led to very noisy results: this happened because, depending on the language considered, the number of tweets can drastically change. To solve this problem, we thought about grouping together tweets of the same week: in this way, we were able to average the results more, get stabler data for languages with fewer tweets, and obtain a clearer visualization.

\begin{figure}[H]
	\dirtree{%
		.1 / .
		.2 en.
		.3 coronavirus-tweet-2020-01-20.json.gz .
		.3 coronavirus-tweet-2020-01-27.json.gz .
		.3 \ldots .
		.3 coronavirus-tweet-2021-03-22.json.gz .
		.3 coronavirus-tweet-2021-03-29.json.gz .
		.2 it .
		.2 \ldots .
	}
	\caption{Weekly tweets organization}
	\label{fig:tweet-org-2}
\end{figure}

In this case, the syntax of the files showed in \Cref{fig:tweet-org-2} has a slightly different meaning. In particular, tweets are grouped into files with a prefix “coronavirus-tweet-id-” followed by YEAR-MONTH-FIRST\_WEEK\_DAY.

\section{Data filtering}
\label{sec:data-filtering}

We have seen in \Cref{sec:period} that the number of retweets in our dataset is much greater than the number of tweets. This could be a problem because, if we perform emotion detection with this data, we may end up with biased results. 

Let us suppose to have a particularly positive tweet, for example:

\begin{figure}[H]
	\centering
    	\includegraphics[scale=.29]{john_doe_tweet.png}
    	\caption{An example of a particularly positive tweet.}
    	\label{fig:tweet-example}
\end{figure}

If John Doe is the only happy person on the 14th of June 2021, then the emotional impact produced from this tweet will not be so significant. Maybe, during that day a lot of people complained on the social network that a streaming platform was not working properly. In this particular case, we will probably find a peak of anger.

However, if John Doe is a celebrity, then the tweet will be surely retweeted thousands of times, and the emotional impact will be much grater. In practice, this means that the previous anger, associated to the malfunctioning of the streaming platform, could pass unnoticed.

To clarify, the fact that a person could retweet a post of a user is not a bad thing: maybe they liked what the original author had written. However, in the case of emotion detection, every words count: if a person agrees with someone's opinion, they will not necessarily use the same exact set of word. In practice, they could still agree with the author of the tweet, but at the same time not convey the same emotion.

For the reasons explained above, we decided to avoid this bias by removing from our dataset all the retweets. As explained in \Cref{sec:tweets}, this particular operation can be performed by checking the presence of the \texttt{retweet\_status} field. 

\section{Emotion detection metrics}
\label{sec:metrics}

We know from \Cref{sec:twitter} that Twitter's constraint on the number of characters per tweet limits users. For this reason, users' emotions are also limited, because they cannot express themselves to the fullest, and, consequently, the emotion detection process could fail, because the small set of word used does not convey any emotion. 

Fortunately, the effect produced from this problem can be minimized with a considerable amount of data. Obviously, with many tweets, the possibility of having meaningful data increases.

However, we have not still defined how users' emotions should be considered: should we count the number of occurrences (i.e. words) for each emotion inside of a tweet? Should we consider independently each tweet posted by a user?

Regarding this particular topic, we have decided to follow one of the approaches discussed by  by Aiello et al.~\cite{aiello2020epidemic}

The idea that they have proposed is to consider
	
\begin{itemize}
	\item emotions in a binary way (e.g. whether at given time the user expressed joy or not)
	\item users over tweets (e.g. the number of unique users, instead of tweets, that expressed joy at a given time)
\end{itemize}

We can then formalize the metrics used for the project as follows:

\begin{definition}
\label{def:tweet-emotions}
	A tweet \(t\) contains an emotion \(e\), if at least one of the tweet's words or stems \(w\) belongs to \(e\),
	\begin{center}
		\(t \in e \Leftrightarrow \exists w \in t\)  s.t. \(w \in e\)
	\end{center}
\end{definition}

First of all, given that there are only 280 characters inside of a tweet, multiple emotions occurrences are rare. Secondly, they do not necessarily reflect the intensity of a tweet. 
	
\begin{definition}
\label{def:user-emotions}
	Given \(U_e(t)\), the number of distinct users that expressed emotion \(e\) at time \(t\) in a tweet, and \(U(t)\), the number of distinct users that tweeted at time \(t\),
	\[f_e(t) = \frac{U_e(t)}{U(t)}\]
	is the proportion of users that expressed emotion \(e\) at time \(t\).	
\end{definition}

Considering the number of unique users, instead of the number of total tweets, is a way to reduce the bias introduced by particularly active users (or bots), who could otherwise mask other important events.

\section{Emotion detection over time - by language}
\label{sec:emotion-by-language}

For the first emotion detection, we have decided to analyze, over the whole period, the course of the users' emotions for a particular language. Specifically, our studies regarded four languages: Catalan, English, Italian and Spanish.

\Cref{tab:tweet-languages} shows the total number of unique tweets and users over the considered period.

\begin{table}[h]
	\centering
	\ra{1.2}
	\begin{tabular}{lrrr}
		\toprule
		\textbf{language} & \textbf{tweets} & \textbf{tweets \%} & \textbf{users}
		\\
		\midrule
		Catalan & 1 377 225 & 0.4\% & 379 193
		\\
		English & 195 645 826 & 60.4\% & 21 979 558 
		\\
		Italian & 5 256 748 & 1.6\% & 558 224
		\\
		Spanish & 35 533 886 & 10.9\% & 5 220 714
		\\
		\bottomrule
	\end{tabular}
	\caption{Number of tweets and users for Catalan, English, Italian and Spanish}
	\label{tab:tweet-languages}
\end{table}

It is important to underline the following: given that, as explained in \Cref{sub:data-org}, we are considering tweets per week, \(f_e(t)\) will correspond to the proportion of users that expressed emotion \(e\) during the analyzed week. For this particular reason, we will keep track of the weekly users and, once the week is over, we will reset them and we will consider a new set of users. In practice, we will aggregate the emotions that a user conveyed only if they tweets multiple times in the same week. 

\subsection{Data normalization}
\label{subsec:normalization}

The issue with the results obtained with this emotion detection, is the fact that the emotions course are not easily comparable. For example, we can notice which emotion was the one expressed more in a certain week considering \(f_e(t)\). However, it is more difficult to say, for example, whether users expressed in a certain week the most joy over the whole period.

To solve this problem, we have decided to apply a data normalization, in particular the \textit{z-score}.

\begin{definition}
	Given \(f_e(t)\) and the period of time \([0,T]\),   
	\[z_e(t) = \frac{f_e(t) - \mu_{[0,T]}(f_e)}{\sigma_{[0,T]}(f_e)}\] 
	\[\text{where } \mu_{[0,T]}(f_e) = \frac{1}{\mid T \mid} \sum_{t =0}^{T} f_e(t) \text{, and } \sigma_{[0,T]}(f_e) = \sqrt{\frac{1}{\mid T \mid} \sum_{t = 0}^{T} \left( f_e(t) - \mu_{[0,T]}(f_e) \right)^2 }\] 
\end{definition}

The z-score is very useful in this particular case because, given the fact that \(f_e(t)\) is normalized w.r.t. its mean value over the whole period, it is easier to determine which weeks have the highest (or lowest) emotional impact. Moreover, this operation is applied to each considered emotion, so we are also able to compare the different courses and determine which emotion has the highest relevance in a certain week.

\section{Emotion detection over time - by gender}
\label{sec:emotion-by-gender}

The second analysis of the data regarded the gender of the users. In particular, we were interested in finding, and understanding, the differences between the emotions expressed by men and women during the considered period of time.

However, Twitter does not allow users to specify their gender explicitly. This means that there is no way to recover this information from a specific field. Besides, searching through the user's description is not a valid option because they may have not given that kind of information. 

Fortunately, even if this information is not explicitly available, is possible to infer it. 

\subsection{	Users inference}
\label{subsec:m3inference}

To solve this particular issue, we used m3inference, a deep learning system for demographic inference implemented on PyTorch~\cite{wang2019demographic}.

Given the user's \texttt{name}, \texttt{screen\_name}, \texttt{description}, \texttt{default\_profile\_image}, and \\
\texttt{profile\_image\_url\_https} fields, m3inference will predict, with a certain probability ([0, 1]), their gender, their age, and if the user's profile belongs to a person or organization.

In particular, m3inference will detect the user's language and then examine their description. Moreover, it will also check if the user has a profile picture with the \texttt{default\_profile\_image} field. If that is the case, it will first download it using the \texttt{profile\_image\_url\_https}, and then it will resize it. Otherwise, it will use the Twitter's default profile image. 

Given the fact that m3inference uses both text and images, operates in 32 major languages, and computes multiple output predictions, is classified respectively as a Multimodal, Multilingual, and Multi-attribute system (M3).

The result of the prediction performed by m3inference is showed in the following example:

\begin{lstlisting}[language=json, caption={Json object returned by m3inference}, captionpos=b, label={lst:m3inference-prediction}]
{
  "gender": {
    "male": 0.8758, 
    "female": 0.1242
  }, 
  "age": {
    "<=18": 0.0053, 
    "19-29": 0.0363, 
    "30-39": 0.9239, 
    ">=40": 0.0346
  }, 
  "org": {
    "non-org": 0.9965, 
    "is-org": 0.0035
  }
}
\end{lstlisting}

Given the fact that the inference process takes some time, it was decided to take into account only a subset of users for each considered language. In particular, we analyzed the users with at least \(n\) tweets over all the dataset. 

Furthermore, for the emotion detection process, we assigned the users that respected \Cref{def:valid-users} to their corresponding category (male, female or org) .

\begin{definition}
\label{def:valid-users}
	A user \(u\) belongs to the category \(c \in C\) iif their prediction confidence \(pc\) is greater or equal than 0.95, i.e.
	\[u \in c \Longleftrightarrow pc(u) \geq 0.95\]
\end{definition}
	
In particular, the following methodology was applied:
	
\begin{itemize}
	\item first we check if the user's account belongs to an organization
	\item then, if the user is male (or female)
	\item finally, if none of the previous constraints were satisfied, we do not consider this user
\end{itemize}

Finally, from the previous considerations we obtained the following statistics:

\begin{table}[h]
	\centering
	\ra{1.2}
	\begin{tabular}{lrrrrrr}
		\toprule
		\textbf{language} & \textbf{\(n\) tweets} & \textbf{inferred users} & \textbf{valid users} & \textbf{males \%} & \textbf{females \%} & \textbf{orgs \%}
		\\
		\midrule
		Catalan & 2 & 98 132 & 73 835 & 67.30 & 22.95 & 9.75
		\\
		English & 10 & 3 099 883 & 2 335 112 & 63.88 & 32.16 & 3.96 
		\\
		Italian & 2 & 217 340 & 167 093 & 67.21 & 27.96 & 4.83 
		\\
		Spanish & 5 & 2 555 941 & 2 029 765 & 63.88 & 33.23 & 2.89 
		\\
		\bottomrule
	\end{tabular}
	\caption{General users statistics for Catalan, English, Italian and Spanish tweets}
	\label{tab:users-languages}
\end{table}
